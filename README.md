## 基于BERT的简单问答系统原型

### 说明

​		BERT是指“基于Transformer的双向编码器”（Bidirectional Encoder Representations from Transformers），是用于自然语言处理（NLP）的预训练模型，由Google在2018年提出。BERT可以比较好的进行语义理解。

​		基于BERT的问答系统，是一个基于所给段落的内容回答问题语言模型，可以用于有预设答案的问答应用。可以对给定的一段文字内容（可以成为“知识”）进行提问，模型可以在给定的“知识”中找到合适的答案。因此在一些有固定答案和可以预设答案的场景，可以使用这个模型进行答案抽取作为提问的答案。

​		当前模型是在中文ALBERT_base基础上使用CMRC2018开源数据集进行finetune训练得到的。

​		因为BERT模型容量和算力的限制，当前模型一次能处理“知识”和问题的文字总长度不能超过512个汉字。为了有效处理更长的文本“知识”，可以采取分段搜索的方式。可以将“知识”分割为若干段，每段不超过模型要求的限制。当提出问题时，使用常见的BM25算法使用问题中的关键字对文本段进行相关性比较，将相关性最大的文本段作为可能的答案所在位置，然后使用BERT模型进行问题抽取。

**模型优点：**

1. 因为是预训练模型，finetune后就可以使用，当“知识”变化时，不用重新训练模型；
2. 基于深度学习的模型，比传统的基于句法和基于统计学的模型效果要好。

**缺点：**

1. 模型性能依赖于finetune的数据集，因此针对特定的问题群，应使用针对性的数据集进行finetune训练；
2. 模型抽取答案的可解释性不强，不能指定回答的答案；
3. 对多段知识的相关性计算，BM25算法最常用，但不是最优，针对特殊应用需要进行算法优化和调整。

**提升方向：**

1. 使用针对性数据集进行finetune训练；
2. 使用大模型进行训练（当前使用的是base，更大模型需要更大算力）；
3. 优化BM25算法；
4. 结合知识图谱方法。



### 本地测试

1. 启动环境
```
make
qa_demo/gotalk/gotalk .
cd src
python3 dispatcher.py 1
uwsgi --http 127.0.0.1:8000  --wsgi-file qa.py --check-static ../
```

2. 在浏览器中打开 ```http://127.0.0.1:8000/static/qa_test.html```
